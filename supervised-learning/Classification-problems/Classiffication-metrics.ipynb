{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574fa728-315d-45d4-9e68-065cb02c9430",
   "metadata": {},
   "source": [
    "## ðŸ§  Classification Metrics Overview\n",
    "\n",
    "Classification metrics help evaluate how well a model performs when predicting categorical (discrete) outcomes, such as spam vs. not spam or disease vs. no disease.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 1. Confusion Matrix\n",
    "\n",
    "A 2x2 matrix for binary classification:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)   | False Negative (FN)  |\n",
    "| **Actual Negative** | False Positive (FP)  | True Negative (TN)   |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 2. Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- Measures overall correctness\n",
    "- Can be **misleading** if classes are imbalanced\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 3. Precision\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- Of all predicted positives, how many were actually correct?\n",
    "- High precision = **low false positive rate**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 4. Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- Of all actual positives, how many were correctly predicted?\n",
    "- High recall = **low false negative rate**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 5. F1 Score\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- Harmonic mean of precision and recall\n",
    "- Best when classes are **imbalanced**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 6. ROC Curve and AUC\n",
    "\n",
    "- **ROC Curve**: Plots True Positive Rate (Recall) vs. False Positive Rate\n",
    "- **AUC (Area Under the Curve)**: Measures the model's ability to distinguish between classes\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¸ 7. Additional Metrics\n",
    "\n",
    "- **Specificity** = \\(\\frac{TN}{TN + FP}\\): True negative rate\n",
    "- **Log Loss**: Penalizes false confident predictions\n",
    "- **Balanced Accuracy**: Average of recall across all classes (good for imbalanced data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81fbb9-4987-42fa-b373-43ef728c282c",
   "metadata": {},
   "source": [
    "#Â Import confusion matrix\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc0da0-1696-4889-a29e-9cbe3eb3785c",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate roc_auc_score\n",
    "print(roc_auc_score(y_test, y_pred_probs))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91963dc-3301-4958-ba16-f5cccb5d13b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
