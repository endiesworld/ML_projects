{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed782ad-b4c6-474d-bd6d-7f1c6bac5dea",
   "metadata": {},
   "source": [
    "# Cross-validation for R-squared\n",
    "\n",
    "\n",
    "Cross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data\n",
    "\n",
    "---\n",
    "## Cross-validation motivation\n",
    "- Model performance is dependent on the way we split up the data\n",
    "- Not representative of the model's ability to generalize to unseen data\n",
    "- Solution: Cross-validation!\r\n",
    "_ _ _\n",
    "## Cross-validation and model performance\n",
    "- 5 folds = 5-fold\n",
    "- CV10 folds = 10-fold\n",
    "- CVk folds = k-fold CV\n",
    "- More folds = More computationally expensive\n",
    "\n",
    "---\n",
    "## Cross-validation in scikit-learn\n",
    "```python\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "    \n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "    reg = LinearRegression()\n",
    "    \n",
    "    cv_results = cross_val_score(reg, X, y, cv=kf)\n",
    "```\n",
    "---\n",
    "## SUPERVISED LEARNING WITH SCIKIT-LEARNER\n",
    "valuating cross-validation performance\n",
    "```python\n",
    "\n",
    "    print(cv_results)\n",
    "    print(np.mean(cv_results), np.std(cv_results))   \n",
    "    print(np.quantile(cv_results, [0.025, 0.975]))\n",
    "    \r\n",
    "\r\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860e0a1-1b8a-41ad-b012-ece752fef6a2",
   "metadata": {},
   "source": [
    "## üîç What is Penalization in Linear Regression?\n",
    "\n",
    "**Penalization** refers to adding a **penalty term** to the loss function in linear regression to discourage overly complex models (e.g., large coefficients). This helps prevent **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why Penalize?\n",
    "\n",
    "In Ordinary Least Squares (OLS), the goal is to minimize(The Residual Sum of Square **RSS**):\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "But without constraints, models can overfit, especially in high-dimensional or noisy data. Penalization adds a cost to large coefficients, helping improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Types of Penalized Regression\n",
    "\n",
    "#### 1. **Ridge Regression (L2 Penalty)**\n",
    "\n",
    "Adds the squared magnitude of coefficients:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Shrinks coefficients smoothly\n",
    "- Keeps all features, but reduces their influence\n",
    "\n",
    "#### 2. **Lasso Regression (L1 Penalty)**\n",
    "\n",
    "Adds the absolute value of coefficients:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "- Encourages **sparsity** (some coefficients become exactly zero)\n",
    "- Useful for **feature selection**\n",
    "\n",
    "#### 3. **Elastic Net**\n",
    "\n",
    "Combines L1 and L2 penalties:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Balances sparsity and shrinkage\n",
    "- Good when features are correlated\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Intuition\n",
    "\n",
    "Penalization changes the optimization objective:\n",
    "\n",
    "- You minimize both prediction error **and** model complexity.\n",
    "- Larger values of **Œª** (lambda) lead to **simpler models**.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a code demo or plot example!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e26a35-7ac6-4e6d-a100-14f0a9e0fb62",
   "metadata": {},
   "source": [
    "## Regularized Regression\n",
    "\n",
    "## Why Regularize?\n",
    "- Recall: Linear regression minimizes a loss function\n",
    "- It chooses a coefficient, a, for each feature variable, plus b\n",
    "- Large coefficients can lead to overfitting\n",
    "- Regularization: Penalize large coefficients\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge Regression\n",
    "- Loss function = OLS loss function +\n",
    "   $$\n",
    "            \\alpha \\cdot \\sum_{i=1}^{n} a_i^2\n",
    "   $$\n",
    "- Ridge penalizes large positive or negative coefficients\n",
    "- $\\alpha$: parameter we need to choose\n",
    "- Picking $\\alpha$ is similar to picking k in KNN\n",
    "- Hyperparameter: variable used to optimize model parameters\n",
    "- $\\alpha$ controls model complexity\n",
    "   - $\\alpha$ = 0 = OLS(Can lead to underfitting)\n",
    "   - Very high $\\alpha$: Can lead to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df887ab8-2543-469b-a449-9b7d791ccc7b",
   "metadata": {},
   "source": [
    "## Ridge regression in scikit-learn\n",
    "``` python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scores = []\n",
    "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:    \n",
    "    ridge = Ridge(alpha=alpha)    \n",
    "    ridge.fit(X_train, y_train)    \n",
    "    y_pred = ridge.predict(X_test)    s\n",
    "    cores.append(ridge.score(X_test, y_test))\n",
    "    \n",
    "print(scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2520990-852a-47e5-9497-7fcbca5e307b",
   "metadata": {},
   "source": [
    "## Lasso regression(L1 Penalty)\n",
    "- Loss function = OLS loss function +\n",
    "   $$\n",
    "            \\alpha \\cdot \\sum_{i=1}^{n} |a_i|\n",
    "   $$\n",
    "---\n",
    "### Lasso regression for feature selection\n",
    "- Lasso can select important features of a dataset\n",
    "- Shrinks the coefficients of less important features to zero\n",
    "- Features not shrunk to zero are selected by lasso\n",
    "\n",
    "---\r\n",
    "\n",
    "### Lasso regression in scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "scores = []for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:  \n",
    "    lasso = Lasso(alpha=alpha)  \n",
    "    lasso.fit(X_train, y_train)  \n",
    "    lasso_pred = lasso.predict(X_test)  \n",
    "    scores.append(lasso.score(X_test, y_test))\n",
    "\n",
    "print(scores)\n",
    "``\n",
    "---\n",
    "\n",
    "### Laso feature selection\n",
    "```python\n",
    "X = diabetes_df.drop(\"glucose\", axis=1).values\n",
    "y = diabetes_df[\"glucose\"].values\n",
    "names = diabetes_df.drop(\"glucose\", axis=1).columns\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "\n",
    "plt.bar(names, lasso_coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871e6d1-4206-4ba8-992a-c708c68b6160",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "+ Ridge/lasso regression: Choosing alpha\n",
    "+ KNN: Choosing n_neighbors\n",
    "+ Hyperparameters: Parameters we specify before fitting the model\n",
    "    - Like alpha and n_neighbors\n",
    " \n",
    "## Choosing the correct hyperparameters\n",
    "1. Try lots of different hyperparameter values\n",
    "2. Fit all of them separately\n",
    "3. See how well they perform\n",
    "4. Choose the best-performing values. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46023c8-3107-4c57-95c0-9bfc3eaa3025",
   "metadata": {},
   "source": [
    "## GridSearchCV in scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\"alpha\": np.arange(0.0001, 1, 10),\"solver\": [\"sag\", \"lsqr\"]}\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "print(ridge_cv.best_params_, ridge_cv.best_score_)\n",
    "```\n",
    "\n",
    "## Limitations and an alternative approach\n",
    "+ 3-fold cross-validation, 1 hyperparameter, 10 total values = 30 fits\n",
    "+ 10 fold cross-validation, 3 hyperparameters, 30 total values = 900 fits\n",
    "\n",
    "## RandomizedSearchCV\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {'alpha': np.arange(0.0001, 1, 10),\"solver\": ['sag', 'lsqr']}\n",
    "ridge = Ridge()\n",
    "ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "print(ridge_cv.best_params_, ridge_cv.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab84894-ab8e-48c4-9b05-a8235a7a1d97",
   "metadata": {},
   "source": [
    "```python\n",
    "#¬†Create the parameter space\n",
    "params = {\"penalty\": [\"l1\", \"l2\"],\n",
    "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
    "         \"C\": np.linspace(0.1, 1.0, 50),\n",
    "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object\n",
    "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
    "\n",
    "# Fit the data to the model\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35efac8f-d04d-46c3-9ef3-c95be614f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00516c42-0bac-4c10-8919-af8e8448dca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
