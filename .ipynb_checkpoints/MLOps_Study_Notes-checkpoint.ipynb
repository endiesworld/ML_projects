{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7364f94",
   "metadata": {},
   "source": [
    "# 📘 MLOps Study Notes: Model Build Pipelines in CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d15e61",
   "metadata": {},
   "source": [
    "## **1. Model Build Pipelines in CI/CD**\n",
    "### 🧱 Two Types of Build Pipelines\n",
    "- **App Build Pipeline**: Standard software build pipeline for serving the model.\n",
    "- **Model Build Pipeline**: Responsible for training the ML model – central to MLOps.\n",
    "\n",
    "### 🔄 Pipeline Terminology\n",
    "- **Model Pipeline**: A sequence of preprocessing steps (e.g., cleaning, feature extraction) ending in a prediction.\n",
    "- **Model BUILD Pipeline**: Automates loading data, training the model, and saving artifacts.\n",
    "\n",
    "### ✅ MLOps-Grade Build Pipeline Requirements\n",
    "- **Deployment readiness**\n",
    "- **Reproducibility**\n",
    "- **Monitoring integration**\n",
    "- **CI/CD compatibility**\n",
    "\n",
    "### 📦 Output of a Good Pipeline\n",
    "- Full model package with:\n",
    "  - Trained model\n",
    "  - Metadata (e.g., data and code versions)\n",
    "  - Dependency specifications\n",
    "  - Monitoring expectations (e.g., data profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3896aad",
   "metadata": {},
   "source": [
    "## **2. Model Packaging**\n",
    "### 📦 What is a Model Package?\n",
    "- A complete bundle required to deploy and monitor a model:\n",
    "  - Trained model file\n",
    "  - Dependency specs\n",
    "  - Data profiles\n",
    "  - Version info\n",
    "\n",
    "### 🗃️ Model Storage Formats\n",
    "- **PMML**: Universal, but less customizable.\n",
    "- **Pickle**: Python-native, highly flexible, but fragile across environments.\n",
    "\n",
    "### 🔁 Reproducibility Essentials\n",
    "- Code and dataset versioning.\n",
    "- Record of performance metrics.\n",
    "- Track train/test splits.\n",
    "\n",
    "### 📊 Monitoring Artifacts\n",
    "- Data profiles capturing expected input/output characteristics.\n",
    "- Used for validating live data against expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1983d",
   "metadata": {},
   "source": [
    "## **3. Model Serving**\n",
    "### 📡 Serving = Providing Predictions as a Service\n",
    "- **API-based model access**\n",
    "- Modes define *when* predictions are made.\n",
    "\n",
    "### 🕒 Serving Modes\n",
    "- **Batch (Offline)**: Periodic, high-throughput, low latency needs.\n",
    "- **On-Demand (Online)**: Immediate predictions based on events.\n",
    "- **Near-Real-Time**: Acceptable latency of minutes (stream processing).\n",
    "- **Real-Time**: Sub-second response time (e.g., fraud detection).\n",
    "- **Edge Deployment**: Model runs on the user's device (e.g., mobile apps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a0309",
   "metadata": {},
   "source": [
    "## **4. Building the API**\n",
    "### 🔌 Exposing the Model\n",
    "- ML server = API endpoint for the model.\n",
    "- Client = App that makes prediction requests.\n",
    "\n",
    "### 🧱 Core Components\n",
    "- **Input Validation**: Check input schema, types, and values.\n",
    "- **Output Validation**: Ensure prediction values are within expected ranges.\n",
    "- **Authentication**: Restrict access to authorized clients.\n",
    "- **Throttling**: Rate-limiting to avoid abuse.\n",
    "\n",
    "### ⚙️ Tools\n",
    "- **FastAPI**: Efficient Python framework for building APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76629a6d",
   "metadata": {},
   "source": [
    "## **5. Testing and Environments**\n",
    "### 🧪 Types of Tests\n",
    "- **Unit Tests**: Test individual functions.\n",
    "- **Integration Tests**: Test modules working together (e.g., API + DB).\n",
    "- **Smoke Tests**: Ensure app starts without crashing.\n",
    "- **Load Tests**: Assess performance under expected traffic.\n",
    "- **Stress Tests**: Push system beyond limits.\n",
    "- **UAT (User Acceptance Testing)**: Validate with real users.\n",
    "\n",
    "### 🌍 Environments\n",
    "- **Development**: Frequent code changes.\n",
    "- **Test**: Stable, used for running tests.\n",
    "- **Staging**: Replica of production.\n",
    "- **Production**: Final live system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3edbd2",
   "metadata": {},
   "source": [
    "## **6. Deployment Strategies**\n",
    "### 🔁 Model Swapping Options\n",
    "- **Blue/Green Deployment**: Switch between two models instantly.\n",
    "- **Canary Deployment**: Gradually roll out new model to a subset of users.\n",
    "- **Shadow Deployment**: Serve old model but send inputs to new one for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16f656",
   "metadata": {},
   "source": [
    "## **7. Monitoring and Alerting**\n",
    "### 📊 Internal Monitoring Focus\n",
    "- Monitor model, pipeline, API, infrastructure.\n",
    "- Granular logs for request/response tracing.\n",
    "\n",
    "### 🧪 Data Validation\n",
    "- Validate input/output data against **data profiles**.\n",
    "- Watch for **data drifts** and **anomalies**.\n",
    "\n",
    "### 🚨 Alerting Best Practices\n",
    "- Avoid alert fatigue by tuning sensitivity.\n",
    "- Alerts must be sent to the right stakeholders promptly.\n",
    "- Maintain logs of incidents and resolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3913b",
   "metadata": {},
   "source": [
    "## **8. Model Maintenance**\n",
    "### 🛠️ Paths to Improvement\n",
    "- **Model-Centric**: Tune architecture, hyperparameters, features.\n",
    "- **Data-Centric**: Improve data quality and labels.\n",
    "\n",
    "### 🧑‍⚕️ Human-in-the-Loop\n",
    "- Leverage human experts to validate predictions and label data.\n",
    "- Each intervention adds new training data.\n",
    "\n",
    "### 📓 Experiment Tracking\n",
    "- Use tools like **MLFlow** to log model versions, datasets, metrics.\n",
    "- Avoid repeating experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5da11e",
   "metadata": {},
   "source": [
    "## **9. Model Governance**\n",
    "### ⚖️ Why Governance?\n",
    "- Prevent risky or unethical model deployments.\n",
    "- Ensure accountability and compliance.\n",
    "\n",
    "### 🧩 Governance Responsibilities\n",
    "- **Design**: Ethics, privacy, bias.\n",
    "- **Development**: Documentation, versioning, quality checks.\n",
    "- **Pre-Production**: API security, monitoring readiness.\n",
    "- **Auditing**: Define roles and responsibilities.\n",
    "\n",
    "### 📛 Risk Categories\n",
    "- Low, Medium, High – based on impact and frequency of model decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e1a18",
   "metadata": {},
   "source": [
    "## **10. Monitoring ML Services**\n",
    "### 🧠 Concept Drift\n",
    "- Occurs when the relationship between inputs and outputs changes.\n",
    "- Can degrade model accuracy over time.\n",
    "\n",
    "### 🛑 Detection Challenges\n",
    "- **Verification Latency**: Ground truth may arrive late or never.\n",
    "- **Covariate Shift**: Input features change independently of output.\n",
    "- **Output Monitoring**: Track distribution changes in predictions."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}