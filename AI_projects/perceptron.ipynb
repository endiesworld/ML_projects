{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f6feb7-97cb-4230-b958-a33709711d0f",
   "metadata": {},
   "source": [
    "# Perceptron ‚Äì The Simplest Neural Network\n",
    "üéØ Objective: <br>\n",
    "Understand how a single-layer neural network (a perceptron) makes binary decisions.\n",
    "$$\r\n",
    "\\hat{y} = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95436e-b780-4d42-9278-909a7548b734",
   "metadata": {},
   "source": [
    "**Where:**<br>\n",
    "- x = [x_1, x_2, ..., x_n] : input vector  \n",
    "- w = [w_1, w_2, ..., w_n] : weights  \n",
    "- b : bias term  \n",
    "- f : activation function (e.g., step function)  \n",
    "- y : predicted output (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dea79-114b-4822-b3ac-d903fe1c0cfa",
   "metadata": {},
   "source": [
    "## What Makes This a Single-Layer Perceptron (SLP)?\n",
    "A single-layer perceptron (SLP) is defined by the number of layers of trainable weights between the input and output.\n",
    "\n",
    "### Key Characteristics of an SLP:\n",
    "**Architecture:** Just one layer of weights connecting input directly to output<br>\n",
    "**Activation Function:** Simple (e.g., step function or sigmoid), applied directly after weighted sum<br>\n",
    "**Training Rule:** Uses Perceptron Learning Rule (or gradient descent if activation is differentiable), There's only one transformation before activation<br>\n",
    "\n",
    "Inputs ‚Üí [ Weights + Bias ] ‚Üí Activation ‚Üí Output\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb6c3da-9a4a-4083-ace6-972730c0fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a485af83-6f60-4aca-91f2-8430b21d60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_dim, lr=0.01, epochs=100):\n",
    "        self.weights = np.zeros(input_dim)\n",
    "        self.bias = 0\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0  # Step function\n",
    "\n",
    "    def predict(self, x):\n",
    "        linear_output = np.dot(self.weights, x) + self.bias\n",
    "        return self.activation(linear_output)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                pred = self.predict(xi)\n",
    "                update = self.lr * (target - pred)\n",
    "                self.weights += update * xi\n",
    "                self.bias += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa090e8-0ddc-4cf3-b572-69d0a4c124ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([0, 0, 0, 1])  # AND function\n",
    "\n",
    "model = Perceptron(input_dim=2)\n",
    "model.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d32df41-94fb-45e1-b2e3-c2193dc52264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predicted: 0\n",
      "Input: [0 1], Predicted: 0\n",
      "Input: [1 0], Predicted: 0\n",
      "Input: [1 1], Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(f\"Input: {x}, Predicted: {model.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201730fa-b5f3-4e01-8835-eadb9a95ff2f",
   "metadata": {},
   "source": [
    "## Perceptron Concept Quiz\n",
    "1. What is the purpose of the activation function in a perceptron?<br>\n",
    "    a) To initialize weights <br>\n",
    "    b) To determine whether the perceptron fires or not <br>\n",
    "    c) To normalize the input features <br>\n",
    "    d) To reduce overfitting <br>\n",
    "\n",
    "2. Which of the following problems cannot be solved by a single-layer perceptron?<br>\n",
    "    a) AND logic gate<br>\n",
    "    b) OR logic gate<br>\n",
    "    c) XOR logic gate<br>\n",
    "    d) NOT logic gate<br>\n",
    "\n",
    "3. What happens during training when the perceptron makes a wrong prediction?<br>\n",
    "    a) The weights remain unchanged<br>\n",
    "    b) The weights are updated to minimize the error<br>\n",
    "    c) A new model is created<br>\n",
    "    d) The perceptron stops training<br>\n",
    "\n",
    "4. Why is a bias term ùëè included in the perceptron?<br>\n",
    "    a) To reduce the dimensionality<br>\n",
    "    b) To scale the output<br>\n",
    "    c) To shift the activation function<br>\n",
    "    d) To prevent overfitting<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a620d06-02db-4c84-8c2a-2fe5870bdc8b",
   "metadata": {},
   "source": [
    "## Answer\n",
    "**Q1: b ‚Äî Correct!** <br>\n",
    "The activation function determines whether the perceptron \"fires\" (outputs a 1) or not (outputs a 0). It's a core part of decision-making in the perceptron.<br>\n",
    "**Q2: c ‚Äî Correct!**<br>\n",
    "A single-layer perceptron cannot solve XOR because XOR is not linearly separable. You need a multi-layer perceptron (MLP) for that.<br>\n",
    "**Q3: b ‚Äî Correct!**<br>\n",
    "When the perceptron makes a wrong prediction, it updates the weights using the Perceptron Learning Rule:<br>\n",
    "**Q5: c ‚Äî Correct!**<br>\n",
    "The bias term allows the activation boundary to shift left or right, improving flexibility in fitting data that isn‚Äôt centered at the origin.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca2a20-7a4c-481f-82d6-bd2ff9a4552c",
   "metadata": {},
   "source": [
    "### Analogy:\n",
    "Think of it like this:<br>\n",
    "**Single-layer perceptron** is like a simple light switch(Digital signal), either on or off, depending on one decision rule.<br>\n",
    "**Multi-layer perceptron (MLP)** is like a smart thermostat(Analog signal), it makes more complex decisions by combining multiple intermediate rules (neurons in hidden layers).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a76899-544c-49ff-8393-ffb887285ea4",
   "metadata": {},
   "source": [
    "## Learning Phase = Transformation Phase\n",
    "In machine learning, especially neural networks, the learning phase refers to the period during which the model:\n",
    "- Sees data\n",
    "- Makes predictions\n",
    "- Calculates errors\n",
    "- Updates parameters (weights and bias)\n",
    "- So in effect, this is when the transformation function:\n",
    "$$\n",
    "\\hat{y} = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$<br>\n",
    "is being tuned so that the output y gets closer to the target y.\n",
    "\n",
    "## Weights and biases are Learned During This Phase\n",
    "- Initially, weights ùë§ and bias ùëè are random (or zero).\n",
    "- During training:\n",
    "    - The model sees inputs and computes outputs.\n",
    "    - It compares outputs to ground truth.\n",
    "    - Based on the error, it updates ùë§ and ùëè to reduce future errors.\n",
    "\n",
    "This happens over multiple epochs.<br>\n",
    "**The learning phase is the transformation phase, and this is also where the appropriate values for weight and bias are determined.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b3eea-f86f-4dc6-9689-8fbd6308fd04",
   "metadata": {},
   "source": [
    "## lr ‚Äî Learning Rate\n",
    "Definition: A small number (e.g., 0.01 or 0.001) that controls how much the model adjusts its weights during each update.<br>\n",
    "**Why It Matters:**<br>\n",
    "It affects how fast or slow your model learns.\n",
    "\n",
    "## epoch ‚Äî Epoch\n",
    "Definition: One full pass through the entire training dataset.<br>\n",
    "**Why It Matters:**\n",
    "The model needs multiple exposures to the data to learn patterns. A single pass is rarely enough.\n",
    "üîÅ Example:\n",
    "You have 1,000 training samples.\n",
    "\n",
    "If you train for 10 epochs, the model will see all 1,000 samples 10 times (in total: 10,000 update opportunities)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc487a-64f8-461d-937f-592829e60f0d",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "Activation functions introduce non-linearity, allowing networks to model complex functions and decision boundaries.\n",
    "Without one, your model would just be a linear transformation no matter how many layers you stack.<br>\n",
    "The activation function used in this case is called the **Heaviside Step Function** (or simply the **Step function**).<br>\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$<br>\n",
    "\n",
    "### Alternatives to the Step Function (Used in Practice)\n",
    "- Sigmoid,     Output Range ‚Üí (0, 1), Good for ‚Üí (Probability) \n",
    "- Tanh,        Output Range ‚Üí (-1, 1), Good for ‚Üí (Zero-centered output) **Better than sigmoid for hidden layers**\n",
    "- ReLu,        Output Range ‚Üí (0, ‚àû), Good for ‚Üí (Fast convergence) **Sparse activations, popular in deep nets**\n",
    "- Leaky ReLu,  Output Range ‚Üí (‚àí‚àû, ‚àû), Good for ‚Üí (Avoids dead neurons) **Fixes ReLU‚Äôs zero-gradient issue**\n",
    "- Softmax,     Output Range ‚Üí (0, 1), sums to 1, Good for ‚Üí (Output layer for multiclass classification) **Converts logits to probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfdd82-05dd-4bd7-b4b5-adf43d1c1065",
   "metadata": {},
   "source": [
    "![Alt text](images/perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "702c5f39-0c58-4b07-912e-ab64d04aac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in AI_projects/.ipynb_checkpoints/perceptron-checkpoint.ipynb.\n",
      "The file will have its original line endings in your working directory\n",
      "warning: LF will be replaced by CRLF in AI_projects/perceptron.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    }
   ],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4dd6dd1c-4b61-47a8-adab-bcaed2da8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main d884488] feat: Added note on neural network\n",
      " 16 files changed, 303 insertions(+)\n",
      " create mode 100644 AI_projects/.ipynb_checkpoints/perceptron-checkpoint.ipynb\n",
      " create mode 100644 AI_projects/.ipynb_checkpoints/perceptron-checkpoint.png\n",
      " create mode 100644 AI_projects/ML_perceptron_1.png\n",
      " create mode 100644 AI_projects/perceptron.ipynb\n",
      " create mode 100644 AI_projects/perceptron.png\n",
      " create mode 100644 AI_projects/perceptron_10.png\n",
      " create mode 100644 AI_projects/perceptron_11.png\n",
      " create mode 100644 AI_projects/perceptron_12.png\n",
      " create mode 100644 AI_projects/perceptron_2.png\n",
      " create mode 100644 AI_projects/perceptron_3.png\n",
      " create mode 100644 AI_projects/perceptron_4.png\n",
      " create mode 100644 AI_projects/perceptron_5.png\n",
      " create mode 100644 AI_projects/perceptron_6.png\n",
      " create mode 100644 AI_projects/perceptron_7.png\n",
      " create mode 100644 AI_projects/perceptron_8.png\n",
      " create mode 100644 AI_projects/perceptron_9.png\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"feat: Added note on neural network\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9cfd413-8e63-4d3d-b675-076dcd9a8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/endiesworld/ML_projects.git\n",
      "   e07a59c..d884488  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeedc19-effc-4aaa-ba3b-34004df45567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
